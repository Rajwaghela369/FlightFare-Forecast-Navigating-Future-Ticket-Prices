{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from requests import RequestException\n",
    "from newspaper import Article, ArticleException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.common.by import By as by\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________________________________________________________________________________\n",
      "Category = World\n",
      "Abstracting links..\n",
      "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,"
     ]
    }
   ],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self):\n",
    "        options = Options()\n",
    "        options.headless = True\n",
    "        self.driver = webdriver.Firefox(executable_path='/Users/rajwaghela/Downloads/Softwares/geckodriver', options=options)\n",
    "        self.category_dict = {}\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        self.driver.get(url='https://news.google.com/home?hl=en-US&gl=US&ceid=US%3Aen')\n",
    "        disable_cookies = WebDriverWait(self.driver, timeout=2).until(\n",
    "            EC.presence_of_element_located((by.XPATH, '/html/body/c-wiz/div/div/div/div[2]/div[1]/div[3]/div[1]/div[1]/form[1]/div/div/button'))\n",
    "        )\n",
    "        disable_cookies.click()\n",
    "        category = self.driver.find_elements_by_xpath('//a[@class=\"brSCsc\"][@role=\"menuitem\"]')\n",
    "        for cat in category:\n",
    "            self.category_dict[cat.text] = cat.get_attribute('href')\n",
    "        self.category_dict = dict(list(self.category_dict.items())[5:])\n",
    "\n",
    "    def run(self):\n",
    "        for cat, url in self.category_dict.items():\n",
    "            print('_____________________________________________________________________________________________________')\n",
    "            print(f'Category = {cat}')\n",
    "            self.driver.get(url)\n",
    "            sleep(3)\n",
    "            self.process_category(cat)\n",
    "\n",
    "    def process_category(self, cat):\n",
    "        tabpanel = self.driver.find_elements_by_xpath('//a[@class=\"WwrzSb\"]')\n",
    "        href = [h.get_attribute('href') for h in tabpanel]\n",
    "        org_urls = []\n",
    "        count = 0 \n",
    "        print('Abstracting links..')\n",
    "        for url in href:\n",
    "            try:\n",
    "                self.driver.get(url)\n",
    "                sleep(1.5)\n",
    "                org_urls.append(self.driver.current_url)\n",
    "                count += 1\n",
    "                print(count, end=',')\n",
    "            except WebDriverException as e:\n",
    "                print(f'Skipping page due to WebDriverException: {e}')\n",
    "                continue\n",
    "        print(f'Total no. of links abstracted ==> {len(org_urls)} ')\n",
    "        total = len(org_urls)\n",
    "        self.fetch_articles(org_urls, cat, total)\n",
    "\n",
    "    def fetch_articles(self, urls, cat, total):\n",
    "        df = {\n",
    "            'Authors':[],\n",
    "            'Pubslish_Date':[],\n",
    "            'Title':[],\n",
    "            'Text':[],\n",
    "            'Image':[],\n",
    "            'Link':[],\n",
    "            'Category':[]\n",
    "        }\n",
    "        count = 0\n",
    "        count_1 = 0\n",
    "        count_2 = 0\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'}\n",
    "        print('Scraping Articles..')\n",
    "        for url in urls:\n",
    "            try:\n",
    "                respones = requests.get(url, headers = headers)\n",
    "                if respones.status_code != 403 or respones.status_code!=404 or respones.status_code!=419 or respones.status_code!=408 or respones.status_code!=406 or respones.status_code!=400 or respones.status_code!=401 or respones.status_code!=203 or respones.status_code!=204 or respones.status_code!=500 or respones.status_code!=502 or respones.status_code!=503 or respones.status_code!=504 :\n",
    "                    if ('Youtube'in url) or ('Yahoo' in url):\n",
    "                        count_1 += 1\n",
    "                    else:\n",
    "                        page = Article(url)\n",
    "                        page.download()\n",
    "                        page.parse()\n",
    "                        df['Authors'].append(page.authors)\n",
    "                        df['Pubslish_Date'].append(page.publish_date)\n",
    "                        df['Title'].append(page.title)\n",
    "                        df['Text'].append(page.text)\n",
    "                        df['Image'].append(page.top_image)\n",
    "                        df['Link'].append(page.canonical_link)\n",
    "                        df['Category'].append(cat)\n",
    "                        count+=1\n",
    "                        print(count, end=',')\n",
    "                else:\n",
    "                    count_2 += 1\n",
    "            except (ArticleException, RequestException) as e:\n",
    "                print(f'\\nError while processing {url}:{e}')\n",
    "                continue\n",
    "        print(f'\\nNo. of non articles links == >{count_1}\\nNo. of Articles Forbid access ==> {count_2}\\nRetrived Articles ==> {total- count_1 - count_2}')\n",
    "\n",
    "        file = pd.DataFrame(df)\n",
    "        file.to_csv(f'Datasets1/{cat}.csv', index=False, header=False, mode='a')\n",
    "        print(f'category {cat} fetched\\ndata imported to path \"Datasets/{cat}.csv\" ')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = NewsScraper()\n",
    "    scraper.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
